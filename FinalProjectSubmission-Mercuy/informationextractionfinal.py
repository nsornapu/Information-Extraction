# -*- coding: utf-8 -*-
"""InformationExtractionFinal.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kpskq6U4-UVB5JBsGGuL8BGsUPZRVmuf

**Project Set up and installation**

Mounting drive
"""

from google.colab import drive
drive.mount('/content/gdrive')

"""Installing and Importing"""

!pip install nltk
import nltk
import re
nltk.download('words')
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')
nltk.download('maxent_ne_chunker')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.stem.porter import PorterStemmer
from nltk.corpus import wordnet
stop_words=set(stopwords.words("english"))
#print(stop_words)
import os
from nltk.parse.stanford import StanfordDependencyParser

path_to_model_jar = "/content/gdrive/My Drive/Colab Notebooks/stanford-english-corenlp-2018-02-27-models.jar"
path_to_jar = "/content/gdrive/My Drive/Colab Notebooks/stanford-parser.jar"
java_path1 = "C://Users/nares/Downloads/_temp_matlab_R2018b_win64/sys/java/jre/win64/jre/bin/java.exe"

def install_java():
  !apt-get install -y openjdk-8-jdk-headless -qq > /dev/null      #install openjdk
  os.environ['JAVAHOME'] = java_path1
  !java -version       #check java version
install_java()

"""Initializing variables"""

VBset = ['VB','VBD','VBG','VBN','VBP','VBZ']
NNSet = ['NN','NNS','NNP','NNPS']
SubjectSet = ['nsubj','nsubjpass']
ObjectSet = ['pobj','dobj','iobj','nmod']
active_subjects=[]
active_objects=[]
passive_objects=[]
max_vbn = []
passive_subjects = []
PersonObjects = []
LocationObjects = []
OrganizationObjects = []
synonyms_marry = [] 
synonyms_kill = [] 
synonyms_born = []
synonyms_travel = []
synonyms_build = []
synonyms_write = [] 
synonyms_divorce = [] 
synonyms_movie = []
synonyms_buy = [] 

def clear_variables():
  active_subjects.clear()
  active_objects.clear()
  passive_objects.clear()
  max_vbn.clear()
  passive_subjects.clear()
  PersonObjects.clear()
  LocationObjects.clear()
  OrganizationObjects.clear()

"""Extracting features"""

def GetFeatures(sentence) :
    featureList = []
    wordList = word_tokenize(sentence)
    #Get Lemma, Stem, hypernym, hyponym,meronym,holonym for each word
    lemmaList = []
    stemList = []
    hypernymList = []
    hyponymList = []
    meronymList = []
    holonymList = []
    for word in wordList :
        lemmaList.append(WordNetLemmatizer().lemmatize(word))
        stemList.append(PorterStemmer().stem(word))
        synSetList =  wordnet.synsets(word)
        if len(synSetList) != 0 :
            selectedWordSynSet = synSetList[0] # use Lesk to get the best synset ( for task 4)
            if (len(selectedWordSynSet.hypernyms()) != 0):
                wordHypernym = selectedWordSynSet.hypernyms()[0]
                if (len(wordHypernym.lemmas()) != 0):
                    hypernymList.append(wordHypernym.lemmas()[0].name())
                else:
                    hypernymList.append("")
            else:
                hypernymList.append("")
            if (len(selectedWordSynSet.hyponyms())!= 0) :
                wordHyponym = selectedWordSynSet.hyponyms()[0]
                if (len(wordHyponym.lemmas())!= 0):
                    hyponymList.append(wordHyponym.lemmas()[0].name())
                else:
                    hyponymList.append("")
            else:
                hyponymList.append("")
            if (len(selectedWordSynSet.part_meronyms()) != 0) :
                wordMeronym = selectedWordSynSet.part_meronyms()[0]
                if (len(wordMeronym.lemmas()) != 0 ) :
                    meronymList.append(wordMeronym.lemmas()[0].name())
                else:
                    meronymList.append("")
            else:
                meronymList.append("")
            if(len(selectedWordSynSet.part_holonyms())!= 0):
                wordHolonym = selectedWordSynSet.part_holonyms()[0]
                if(len(wordHolonym.lemmas())!=0):
                    holonymList.append(wordHolonym.lemmas()[0].name())
                else:
                    holonymList.append("")
            else:
                holonymList.append("")
        else:
            hypernymList.append("")
            hyponymList.append("")
            meronymList.append("")
            holonymList.append("")
    featureList.append(lemmaList)
    featureList.append(stemList)
    #test
    #print (sentence)
    #print (lemmaList)
    #print (stemList)
    #Get POS for each word
    taggedWordList = nltk.pos_tag(wordList)
    posList = []
    for taggedWord in taggedWordList :
        posList.append(taggedWord[1])
    featureList.append(posList)
    #test
    #print (posList)
    #Get HeadWord for the sentence
    headWord = ""
    striped_sentence = sentence.strip(" '\"")
    if len(striped_sentence) != 0 :
        dependency_parser = StanfordDependencyParser(path_to_jar, path_to_model_jar)
        result = dependency_parser.raw_parse(striped_sentence)
        parseTree = list(result)[0]
        for n in parseTree.nodes.values():
            if n['head'] == 0 :
                headWord = n['word']
                break
    featureList.append(headWord)
    #test
    #print (headWord)
    #add hypernym,hyponym,meronym and holonym lists to feature list
    featureList.append(hypernymList)
    featureList.append(hyponymList)
    featureList.append(meronymList)
    featureList.append(holonymList)
    #test
    print("hypernym list :"+str(hypernymList))
    print("hyponymList list :"+str(hyponymList))
    print("meronymList list :"+str(meronymList))
    print("holonymList list :"+str(holonymList))

    #synonyms for task 4.

    return featureList

"""Functions definitions"""

def tokenize_input(input):
  tokenized_word=word_tokenize(input)
  return tokenized_word

def filtered_input(input):
  filtered_word=[]
  for w in input:
      if w not in stop_words:
          filtered_word.append(w)
  return filtered_word

def lemmatize_input(input):
  lem = WordNetLemmatizer()
  lemmatized_output = []
  for word in input:
    lemmatized_output.append(lem.lemmatize(word,"v"))
  return lemmatized_output

def part_of_speech_tagging(input):
  tagged = nltk.pos_tag(input)
  return tagged

'''def named_entity_recognition(input):
  output = []
  for chunk in nltk.ne_chunk(input):
    if hasattr(chunk, 'label'):
      print(chunk.label(), ' '.join(c[0] for c in chunk))
      output.append([chunk.label(), ' '.join(c[0] for c in chunk)])
  return output 
'''

def named_entity_recognition(input):
  output = []
  for chunk in nltk.ne_chunk(input):
    if hasattr(chunk, 'label'):
      #print(chunk.label(), ' '.join(c[0] for c in chunk))
      output.append([chunk.label(),"".join(c[0] for c in chunk)])
  return output
'''
def named_entity_recognition_all(input):
  output = []
  for chunk in nltk.ne_chunk(input):
      output.append([chunk.label(),"".join(c[0] for c in chunk)])
  return output
'''

def search_date(input):
  matchObj = re.search( r'(?i)((([1-9]|1[0-9]|2[0-9]|3[0-1])(st|th|rd)?(.|-| )([1-9] |1[0-2])(.|-| )((19|20)[0-9][0-9])?)|(([1-9]|1[0-9]|2[0-9]|3[0-1])(st|th|rd)?(.|-| )(January|February|March|April|May|June|July|August|September|October|November|December)(.|-| )?((19|20)[0-9][0-9])?)|(([1-9]|1[0-9]|2[0-9]|3[0-1])(st|th|rd)?(.|-| )(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)(.|-| )?((19|20)[0-9][0-9])?))', input, re.M|re.I)
  if matchObj:
    return matchObj.group()
  else:
    return
def search_price(input):
  matchObj = re.search( r'(?i)(INR|USD)?(.|-| )?[0-9]+(.)?[0-9]+(.|-| )(crore|lakh|million)?(INR|USD)?', input, re.M|re.I)
  if matchObj:
    return matchObj.group()
  else:
    return
  
def dependecy_parsing(input):
  dependency_parser = StanfordDependencyParser(path_to_jar, path_to_model_jar)
  result = dependency_parser.raw_parse(input)
  dep = next(result)
  return list(dep.triples())

def most_common(lst):
  return max(set(lst), key=lst.count)

def find_subject_vb(input):

  for i in range(len(dependecy_parsing(input))):
    if(dependecy_parsing(input)[i][1] in SubjectSet):
      if(dependecy_parsing(input)[i][1] == 'nsubjpass'):
        passive_subjects.append(dependecy_parsing(input)[i][2][0])
      else:
        active_subjects.append(dependecy_parsing(input)[i][2][0])
    if(dependecy_parsing(input)[i][0][1] in VBset):
      max_vbn.append(dependecy_parsing(input)[i][0][0])
    if(dependecy_parsing(input)[i][1] in ObjectSet):
      if(dependecy_parsing(input)[i][1] == 'nmod'):
        passive_objects.append(dependecy_parsing(input)[i][2][0])
      else:
        active_objects.append(dependecy_parsing(input)[i][2][0])
        
def named_entity_person_recognition(input):
  output = []
  for chunk in nltk.ne_chunk(input):
    if hasattr(chunk, 'label'):
      if (chunk.label() == 'PERSON'):
      #print(chunk.label(), ' '.join(c[0] for c in chunk))
        output.append([chunk.label(),"".join(c[0] for c in chunk)])
  return output
def named_entity_location_recognition(input):
  output = []
  for chunk in nltk.ne_chunk(input):
    if hasattr(chunk, 'label'):
      if (chunk.label() == 'GPE'):
      #print(chunk.label(), ' '.join(c[0] for c in chunk))
        output.append([chunk.label(),"".join(c[0] for c in chunk)])
  return output

def named_entity_organization_recognition(input):
  output = []
  for chunk in nltk.ne_chunk(input):
    if hasattr(chunk, 'label'):
      if (chunk.label() == 'ORGANIZATION'):
      #print(chunk.label(), ' '.join(c[0] for c in chunk))
        output.append([chunk.label(),"".join(c[0] for c in chunk)])
  return output

"""find template"""

def find_template_based_synonyms():
  word = "marry"
  for syn in wordnet.synsets(word): 
      for l in syn.lemmas(): 
          synonyms_marry.append(l.name())  
  #print("synonyms of word "+word + " are "+ str(synonyms_marry))

  word = "kill"
  for syn in wordnet.synsets(word): 
      for l in syn.lemmas(): 
          synonyms_kill.append(l.name())  
  synonyms_kill.append('murder')
  synonyms_kill.append('assassinate'),
  synonyms_kill.append('execute')
  synonyms_kill.append('eliminate')
  #print("synonyms of word "+word + " are "+ str(synonyms_kill))

  word = "born"
  for syn in wordnet.synsets(word): 
      for l in syn.lemmas(): 
          synonyms_born.append(l.name())  
  #print("synonyms of word "+word + " are "+ str(synonyms_born))
  

  word = "movie"
  for syn in wordnet.synsets(word): 
      for l in syn.lemmas(): 
          synonyms_movie.append(l.name())  
  synonyms_movie.append('film')
  synonyms_movie.append('cinema')
  synonyms_movie.append('picture')
  synonyms_movie.append('motion picture')
  synonyms_movie.append('feature')
  #print("synonyms of word "+word + " are "+ str(synonyms_movie))

  word = "travel"
  for syn in wordnet.synsets(word): 
      for l in syn.lemmas(): 
          synonyms_travel.append(l.name())  
  synonyms_travel.append('journey')
  synonyms_travel.append('tour')
  synonyms_travel.append('trip')
  synonyms_travel.append('voyage')
  synonyms_travel.append('explore')
  #print("synonyms of word "+word + " are "+ str(synonyms_travel))

  word = "build"
  for syn in wordnet.synsets(word): 
      for l in syn.lemmas(): 
          synonyms_build.append(l.name())  
  synonyms_build.append('built')
  synonyms_build.append('contruct')
  synonyms_build.append('put up')
  #print("synonyms of word "+word + " are "+ str(synonyms_build))

  word = "write"
  for syn in wordnet.synsets(word): 
      for l in syn.lemmas(): 
          synonyms_write.append(l.name())  
  synonyms_write.append('wrote')
  #print("synonyms of word "+word + " are "+ str(synonyms_write))
  
  word = "divorce"
  for syn in wordnet.synsets(word): 
      for l in syn.lemmas(): 
          synonyms_divorce.append(l.name())  
  synonyms_divorce.append('separation')
  #print("synonyms of word "+word + " are "+ str(synonyms_divorce))
  word = "buy"
  for syn in wordnet.synsets(word): 
      for l in syn.lemmas(): 
          synonyms_buy.append(l.name())  
  synonyms_buy.append('sold')
  synonyms_buy.append('bid')
  synonyms_buy.append("sell")
  #print("synonyms of word "+word + " are "+ str(synonyms_buy))
find_template_based_synonyms()

def find_template(input_sentence):
  temp = ""
  if ("run" in lemmatize_input(tokenize_input(input_sentence))):
    print("template cricket commentary")
    cricket_commentary_template(input_sentence)
  for temp in synonyms_movie:
    if (temp in lemmatize_input(tokenize_input(input_sentence))):
      print("template movie")
      find_subject_vb(input_sentence)
      movie_template(input_sentence)
      return
  else:
    find_subject_vb(input_sentence)
    lem = WordNetLemmatizer()
    lem.lemmatize("killed","v")
    for obj in set(max_vbn):
      temp = lem.lemmatize(obj,"v")
      #print(temp)
      if (lem.lemmatize(obj,"v") in synonyms_buy):
        print("template cricket auction")
        auction_template(input_sentence)
      if (lem.lemmatize(obj,"v") in synonyms_marry):
        print("template marry")
        marry_template(input_sentence)
      if (lem.lemmatize(obj,"v") in synonyms_kill):
        print("template kill")
        kill_template(input_sentence)
      if (lem.lemmatize(obj,"v") in synonyms_born):
        print("template born")
        born_template(input_sentence)
      if (lem.lemmatize(obj,"v") in synonyms_travel):
        print("template travel")
        travel_template(input_sentence)
      if (lem.lemmatize(obj,"v") in synonyms_build):
        print("template build")
        built_template(input_sentence)
      if (lem.lemmatize(obj,"v") in synonyms_write):
        print("template write")
        write_template(input_sentence)
      if (lem.lemmatize(obj,"v") in synonyms_divorce):
        print("template divorce")
        divorce_template(input_sentence)
  #clear_variables()

def kill_template(input):
  tokenize_input(input)
  killer = ""
  killed = ""
  location = ""
  namedEnt = nltk.ne_chunk(part_of_speech_tagging(filtered_input(tokenize_input(input))))
  #print(namedEnt)

  filtered_input(tokenize_input(input))

  lemmatize_input(filtered_input(tokenize_input(input)))
  part_of_speech_tagging(tokenize_input(input))
  #print(named_entity_recognition(part_of_speech_tagging(filtered_input(tokenize_input(input)))))
  NEObjects = []
  for i in range(len(named_entity_recognition(part_of_speech_tagging(filtered_input(tokenize_input(input)))))):
    NEObjects.append(named_entity_recognition(part_of_speech_tagging(filtered_input(tokenize_input(input))))[i][1])
  for i in range(len(named_entity_person_recognition(part_of_speech_tagging(tokenize_input(input))))):
    PersonObjects.append(named_entity_person_recognition(part_of_speech_tagging(tokenize_input(input)))[i][1])
  for i in range(len(named_entity_location_recognition(part_of_speech_tagging(tokenize_input(input))))):
    LocationObjects.append(named_entity_location_recognition(part_of_speech_tagging(tokenize_input(input)))[i][1])
  date = search_date(input)
  #print(len(active_subjects)==0)
  #find_subject_vb(input)
  if(len(active_subjects)==0):
    for obj in passive_objects:
      #print(obj)
      for i in range(len(part_of_speech_tagging(filtered_input(tokenize_input(input))))):
        if(obj==part_of_speech_tagging(filtered_input(tokenize_input(input)))[i][0] and (part_of_speech_tagging(filtered_input(tokenize_input(input)))[i][1] in NNSet) and obj in PersonObjects):
          #print(obj)
          killer = obj
    for obj in passive_subjects:
      for i in range(len(part_of_speech_tagging(filtered_input(tokenize_input(input))))):
        if(obj==part_of_speech_tagging(filtered_input(tokenize_input(input)))[i][0] and (part_of_speech_tagging(filtered_input(tokenize_input(input)))[i][1] in NNSet)):
          #print(obj)
          killed = obj
    for obj in PersonObjects:
        if(obj in passive_objects and obj in LocationObjects):
          #print(obj)
          location = obj
    print("input: "+input)
    print("KILLS("+killer+","+killed+","+date+","+str(LocationObjects)+")")
  else:
    for obj in active_objects:
      #print(obj)
      for i in range(len(part_of_speech_tagging(filtered_input(tokenize_input(input))))):
        if(obj==part_of_speech_tagging(filtered_input(tokenize_input(input)))[i][0] and (part_of_speech_tagging(filtered_input(tokenize_input(input)))[i][1] in NNSet) and (obj not in NEObjects)):
          #print(obj)
          killed = obj
    for obj in active_subjects:
      for i in range(len(part_of_speech_tagging(filtered_input(tokenize_input(input))))):
        if(obj==part_of_speech_tagging(filtered_input(tokenize_input(input)))[i][0] and (part_of_speech_tagging(filtered_input(tokenize_input(input)))[i][1] in NNSet)):
          #print(obj)
          killer = obj
    for obj in NEObjects:
      if(obj in passive_objects and obj in LocationObjects):
        #print(obj)
        location = obj
  #def kill_template(input):
    print("input: "+input)
    print("KILLS("+killer+","+killed+","+date+","+str(LocationObjects)+")")

def born_template(input):
  #tokenize_input(input)
  subject = ""
  #namedEnt = nltk.ne_chunk(part_of_speech_tagging(tokenize_input(input)))
  #print(namedEnt)

  #filtered_input(tokenize_input(input))

  #lemmatize_input(tokenize_input(input))
  #part_of_speech_tagging(tokenize_input(input))
  #print(named_entity_recognition(part_of_speech_tagging(filtered_input(tokenize_input(input)))))
  for i in range(len(named_entity_person_recognition(part_of_speech_tagging(tokenize_input(input))))):
    PersonObjects.append(named_entity_person_recognition(part_of_speech_tagging(tokenize_input(input)))[i][1])
  for i in range(len(named_entity_location_recognition(part_of_speech_tagging(tokenize_input(input))))):
    LocationObjects.append(named_entity_location_recognition(part_of_speech_tagging(tokenize_input(input)))[i][1])
  date = search_date(input)
  #print(len(active_subjects)==0)
  #find_subject_vb(input)
  #if(len(active_subjects)==0):
  for obj in passive_subjects:
    #print(obj)
    subject = obj
    PersonObjects.remove(subject)
  print("input: "+input)
  print("BORN("+subject+","+str(PersonObjects)+","+date+","+str(LocationObjects)+")")

def auction_template(input):
  #tokenize_input(input)
  #subject = ""
  #namedEnt = nltk.ne_chunk(part_of_speech_tagging(tokenize_input(input)))
  #print(namedEnt)
  #filtered_input(tokenize_input(input))
  #lemmatize_input(tokenize_input(input))
  #part_of_speech_tagging(tokenize_input(input))
  #date = search_date(input)
  for i in range(len(named_entity_person_recognition(part_of_speech_tagging(tokenize_input(input))))):
    PersonObjects.append(named_entity_person_recognition(part_of_speech_tagging(tokenize_input(input)))[i][1])
  for i in range(len(named_entity_location_recognition(part_of_speech_tagging(tokenize_input(input))))):
    LocationObjects.append(named_entity_location_recognition(part_of_speech_tagging(tokenize_input(input)))[i][1])
  for i in range(len(named_entity_location_recognition(part_of_speech_tagging(tokenize_input(input))))):
    OrganizationObjects.append(named_entity_organization_recognition(part_of_speech_tagging(tokenize_input(input)))[i][1])
  if ('INR' in OrganizationObjects):
    OrganizationObjects.remove('INR')
  #print(len(active_subjects)==0)
  '''find_subject_vb(input)
  #if(len(active_subjects)==0):
  for obj in passive_subjects:
    #print(obj)
    subject = obj
    PersonObjects.remove(subject)'''
  print("input: "+input)
  print("AUCTION("+str(PersonObjects)+","+str(OrganizationObjects)+","+str(LocationObjects)+","+search_price(input)+")")

def cricket_commentary_template(input):
  matchObj = re.search( r'(?i)(\w+)\s+to\s+(\w+),?\s+(\w+)\s+runs?,?\s+(\d+)\s*kph?', input, re.M|re.I)
  print("input :"+input)
  print("SCOREUPDATE("+matchObj.group(2)+","+matchObj.group(1)+","+matchObj.group(3)+" runs,"+matchObj.group(4)+"kph)")

def movie_template(input):
  genre = re.findall( r'(?i)(Action|Adventure|Comedy|Drama|Crime|Horror|Fantasy|Romance|Thriller|Animation|Family|War|superhero|Sci-Fi)', input, re.M|re.I)
  year = re.search( r'(?i)(\d{4})', input, re.M|re.I)
  for i in range(len(named_entity_location_recognition(part_of_speech_tagging(tokenize_input(input))))):
    OrganizationObjects.append(named_entity_organization_recognition(part_of_speech_tagging(tokenize_input(input)))[i][1])
  print("input :"+input)
  print("MOVIE("+str(active_subjects)+","+str(genre)+","+str(year)+str(OrganizationObjects)+")")

def travel_template(input):
  for i in range(len(named_entity_person_recognition(part_of_speech_tagging(tokenize_input(input))))):
    PersonObjects.append(named_entity_person_recognition(part_of_speech_tagging(tokenize_input(input)))[i][1])
  for i in range(len(named_entity_location_recognition(part_of_speech_tagging(tokenize_input(input))))):
    LocationObjects.append(named_entity_location_recognition(part_of_speech_tagging(tokenize_input(input)))[i][1])
  date = search_date(input)
  #print(len(active_subjects)==0)
  #find_subject_vb(input)
  #i(len(active_subjects)==0):
  for obj in active_subjects:
    #print(obj)
    if (obj in LocationObjects):
      LocationObjects.remove(obj)
  #for 
  print("input: "+input)
  print("TRAVEL("+str(active_subjects)+","+str(LocationObjects)+","+date+")")

def marry_template(input):
  for i in range(len(named_entity_person_recognition(part_of_speech_tagging(tokenize_input(input))))):
    PersonObjects.append(named_entity_person_recognition(part_of_speech_tagging(tokenize_input(input)))[i][1])
  for i in range(len(named_entity_location_recognition(part_of_speech_tagging(tokenize_input(input))))):
    LocationObjects.append(named_entity_location_recognition(part_of_speech_tagging(tokenize_input(input)))[i][1])
  date = search_date(input)
  #print(len(active_subjects)==0)
  #find_subject_vb(input)
  #i(len(active_subjects)==0):
  for obj in active_subjects:
    #print(obj)
    if (obj in PersonObjects):
      PersonObjects.remove(obj)
  #for 
  print("input: "+input)
  print("MARRY("+str(active_subjects)+","+str(PersonObjects)+","+str(LocationObjects)+","+date+")")

def built_template(input):
  NEObjects = []
  for i in range(len(named_entity_recognition(part_of_speech_tagging(filtered_input(tokenize_input(input)))))):
    NEObjects.append(named_entity_recognition(part_of_speech_tagging(filtered_input(tokenize_input(input))))[i][1])
  for i in range(len(named_entity_person_recognition(part_of_speech_tagging(tokenize_input(input))))):
    PersonObjects.append(named_entity_person_recognition(part_of_speech_tagging(tokenize_input(input)))[i][1])
  for i in range(len(named_entity_location_recognition(part_of_speech_tagging(tokenize_input(input))))):
    LocationObjects.append(named_entity_location_recognition(part_of_speech_tagging(tokenize_input(input)))[i][1])
  year = re.search( r'(?i)(\d{4})', input, re.M|re.I)
  person = ""
  building=""
  if(len(active_subjects)==0):
    for obj in passive_objects:
      #print(obj)
      for i in range(len(part_of_speech_tagging(filtered_input(tokenize_input(input))))):
        if(obj==part_of_speech_tagging(filtered_input(tokenize_input(input)))[i][0] and (part_of_speech_tagging(filtered_input(tokenize_input(input)))[i][1] in NNSet) and obj in PersonObjects):
          #print(obj)
          person = obj
    for obj in passive_subjects:
      for i in range(len(part_of_speech_tagging(filtered_input(tokenize_input(input))))):
        if(obj==part_of_speech_tagging(filtered_input(tokenize_input(input)))[i][0] and (part_of_speech_tagging(filtered_input(tokenize_input(input)))[i][1] in NNSet)):
          #print(obj)
          building = obj
    print("input: "+input)
    print("BUILT("+person+","+building+","+str(LocationObjects)+","+str(year.group())+")")
  else:
    for obj in active_objects:
      #print(obj)
      for i in range(len(part_of_speech_tagging(filtered_input(tokenize_input(input))))):
        if(obj==part_of_speech_tagging(filtered_input(tokenize_input(input)))[i][0] and (part_of_speech_tagging(filtered_input(tokenize_input(input)))[i][1] in NNSet)):
          #print(obj)
          building = obj
    for obj in active_subjects:
      for i in range(len(part_of_speech_tagging(filtered_input(tokenize_input(input))))):
        if(obj==part_of_speech_tagging(filtered_input(tokenize_input(input)))[i][0] and (part_of_speech_tagging(filtered_input(tokenize_input(input)))[i][1] in NNSet)):
          #print(obj)
          person = obj
    print("input: "+input)
    print("BUILT("+person+","+building+","+str(LocationObjects)+","+str(year.group())+")")

def divorce_template(input):
  for i in range(len(named_entity_person_recognition(part_of_speech_tagging(tokenize_input(input))))):
    PersonObjects.append(named_entity_person_recognition(part_of_speech_tagging(tokenize_input(input)))[i][1])
  for i in range(len(named_entity_location_recognition(part_of_speech_tagging(tokenize_input(input))))):
    LocationObjects.append(named_entity_location_recognition(part_of_speech_tagging(tokenize_input(input)))[i][1])
  date = search_date(input)
  #print(len(active_subjects)==0)
  #find_subject_vb(input)
  #i(len(active_subjects)==0):
  for obj in active_subjects:
    #print(obj)
    if (obj in PersonObjects):
      PersonObjects.remove(obj)
  #for 
  matchObj = re.search( r'(?i)(\w+)\s+years?', input, re.M|re.I)
  print("input: "+input)
  print("DIVORCE("+str(active_subjects)+","+str(PersonObjects)+","+str(LocationObjects)+","+date+","+str(matchObj.group())+")")

def write_template(input):
  genre = re.findall( r'(?i)(Action|Adventure|Comedy|Drama|Crime|Horror|Fantasy|Romance|Thriller|Animation|Family|War|superhero|Sci-Fi)', input, re.M|re.I)
  year = re.search( r'(?i)(\d{4})', input, re.M|re.I)
  for i in range(len(named_entity_location_recognition(part_of_speech_tagging(tokenize_input(input))))):
    OrganizationObjects.append(named_entity_organization_recognition(part_of_speech_tagging(tokenize_input(input)))[i][1])
  for i in range(len(named_entity_person_recognition(part_of_speech_tagging(tokenize_input(input))))):
    PersonObjects.append(named_entity_organization_recognition(part_of_speech_tagging(tokenize_input(input)))[i][1])
  print("input :"+input)
  print("MOVIE("+str(PersonObjects)+","+str(genre)+","+str(year)+str(OrganizationObjects)+")")

#max_vbn
input_sentence = "Roddick was born the youngest of three boys in Omaha, Nebraska, the son of Blanche (née Corell), a school teacher, and Jerry Roddick, a businessman, on 30 April 1987"
clear_variables()
find_template(input_sentence)



input_sentence = "RCB enter the bidding, against Mumbai. The bid is with RCB at INR 85 lakh. Kulwant Khejroliya sold to RCB"
clear_variables()
find_template(input_sentence)

input_sentence = "Starc to Sharma, 1 run, 146kph, inswinging yorker, he jams the bat down and squeezes it behind square, just got the bat down in time!"
clear_variables()
find_template(input_sentence)

input_sentence = "Deadpool is a 2016 American superhero film based on the Marvel Comics character of the same name, distributed by 20th Century Fox."
clear_variables()
find_template(input_sentence)

input_sentence = "Naresh is travelling from Dallas, Texas to Bangalore, India on 17th December."
clear_variables()
find_template(input_sentence)

input_sentence = "After spending 5 years together, Rohit and Ritika decided to get married on 12th February 2018 in Mumbai, India"
clear_variables()
find_template(input_sentence)

input_sentence = "After spending 5 years together, Rohit and Ritika decided to get divorced on 12th February 2018 in Mumbai, India"
clear_variables()
find_template(input_sentence)

input_sentence = "ShahJahan built TajMahal in 1632 and located in Agra, India"
clear_variables()
find_template(input_sentence)

input_sentence = "GOT is a series of epic fantasy novels written by the American novelist and screenwriter George R. R. Martin in 1996"
clear_variables()
find_template(input_sentence)

input = "Gandhi was assassinated by Godse, on 30 January 1948 in the compound of Birla House (now Gandhi Smriti), a large mansion in New Delhi India."
clear_variables()
find_template(input_sentence)

